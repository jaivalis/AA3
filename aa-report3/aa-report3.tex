\documentclass[a4paper,10pt]{article}
\usepackage{array}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgfplotstable}
\usepackage{pgfplots}
\usepackage{filecontents}
\usepackage{amsmath}
\usepackage{float}



\title{
	\textbf{Autonomous Agents Assignment 3}
}

\author{Tobias Stahl \\ 10528199 \and Spyros Michaelides \\ 10523316 \and Ioannis Giounous Aivalis \\ 10524851 \and Francesco Stablum \\ 6200982}




\begin{document}

\maketitle


\section{Introduction}
In this assignment, an already implemented environment setting is going to initially be updated in order to test the behaviour of the algorithms previously implemented, in situations with more than one predator, trying to catch in this case an an intelligent prey. This new environment setting will be able to host up to 3 predators, with the fourth agent being the intelligent prey. Random policies will initially be used to check if the environment works correctly..
Following the experimentation with this new environment, a different algorithm named minimax-Q will be implemented, using elements from Q-learning, and the minimax decision rule in game theory.
Finally other learning algorithms for Markov games will be implemented and tested, in order to analyse their performance.






%--A general framework for explaining your results/experiments is to follow these points : 

%HYPOTHESIS
%--1) why? "in order to test..." (questions, and preferably hypotheses with explanation) --> Hypothesis
%--2) is there anything to mention about the implementation/machinery the reader should know? (e.g. "these experiments were performed on a [insert machine specs]" when presenting runtimes) - 

%RESULTS

%Interpretation
%--3) what does it show (and is that what you expected, and why)

%Findings
%--4) take home message (what do you want the reader to remember, e.g. "Therefore, by reducing the state space, we have gained several orders of magnitude in runtime.")


\section{New Environment}
\subsection{Summary}
The previously implemented environment included a prey which acted randomly with options to either move between 4 different directions with a probability of 5\%, or to stay put, by not moving, with a probability of 80\%. The predator was the only agent in the environment and always moved prior to the prey, whilst the prey was never allowed to move into the coordinates of where the predator stands (since that would signify the termination of the chase).\\
In the newly implemented environment, both the prey and the predator(s) will move at the same time, meaning that if the predator and prey move onto the same coordinates, the predator will instantly get a reward of +10 for and the prey will receive a reward of -10, resulting in a terminated episode. Any other transition would result in a reward of 0 for the agents.
The prey will now be also be an agent with slightly different movement options, with an 80\% chance in moving using any of the  four given directions, whilst also having a 20\% chance of tripping, in which case the prey would stay put. The prey being an agent now too, will also have the algorithms applied in order to make it 'intelligent', and hence harder to catch by the predator(s). 
In the case where more than one predator is in the environment, there is the possibility that the predators will move into each other. In doing so, the predators will each receive a reward of -10 and the episode will also end.
%intro, summary

\subsection{Experiment}


\subsubsection{Hypothesis}


\subsubsection{Results}
%Interpretation


%Findings






\section{Independent Q-Learning}
\subsection{Summary}
%intro, summary
Q-Learning is an algorithm designed to work in a Markov Decision Process (MDP), for an underlying transition model that does not change with time. The algorithm estimates the the optimal policy, without the transition model of the environment, by repeatedly interacting with the environment and trying to estimate the optimal state/action values through trial and error. Unlike MDPs, in Markov Games, the environment is extended to host a multiple number of agents, which means that the transition model depends on the joint action of the agents, and each agent receives a different reward as a result of this joint action.\\
Q-learning could be implemented on an agent in a Markov game, by treating the all other agents as part of the environment. Theoretically, having multiple agents using Q-learning to learn, should result in a poor performance, since the transition model $p(state'|state, action_{i})$ of the agent $i$ will constantly be changing due to the policies of the other agents who are also learning simultaneously. This approach, in turn, is named 'Independent Q-learning' since each agent in the simulation will be independently learning the optimal Q-values, and hence policy, of the underlying (changing) transition model of the environment.


\subsection{Experiment}


\subsubsection{Hypothesis}


\subsubsection{Results}
%Interpretation


%Findings






\section{minimax-Q}
%intro, summary
\subsection{Summary}
The following summary introduces briefly the consequences of using the Markov game framework in place of MDP's in reinforcement learning for two-player zero-sum games.
Since in Markov games the best choice of action depends on the opponent there is no undominated policy, so the problem could be resolved using components from game-theory. Adopting this to the two agent problem, the solution could be approached by evaluating each policy with respect to the scenario where the opponent acts in the worst possible way for the agent, i.e. the opponent tries to minimize the reward function of the agent. Based on the opponent's strategy, the agent will attempt to select the policy that maximizes the discounted reward. Taking into consideration the the action of the opponent as $o \in O$, the value of a state in a Markov game can be redefined as the expected reward for the optimal policy starting from a state $s$, and the quality $Q(s,a,o)$ as the expected reward for taking action $a$ when the opponent chooses $o$, and continuing optimally afterwards. \\
Minimax-Q works in the same way as Q-learning, with the difference that it takes as an action one that minimizes the possible loss for a worst case scenario under the opponent's action o instead of just the maximum approximated action value (without considering the opponents action).
Littman in his paper implements an experiment with two agents 'playing football' on a 4x5 grid, where the aim is for the agent with the ball to move into the correct goal post. Littman implements a series of experiments to compare the performance of minimax-Q in comparison to Q-learning, using different training methods (learning against a random opponent or the algorithm itself.) The results are that minimax-Q allows the agent to converge to a risk averse strategy, due to the fact that it constantly takes into consideration the worst possible action the opponent could take. 


\subsection{Experiment}



\subsubsection{Hypothesis}


\subsubsection{Results}
%Interpretation


%Findings








\section{Other Learning Algorithms}



\subsection{Experiment}



\subsubsection{Hypothesis}


\subsubsection{Results}
%Interpretation


%Findings





\section{Discussion}



\section{Conclusion}


\begin{thebibliography}{2}

\bibitem{sutton}
  Richard S. Sutton and Andrew G. Barto ,
  \emph{Reinforcement Learning: An Introduction}.
  The MIT Press, Cambridge, Massachusetts


\bibitem{vlassi}
  Nikos Vlassis,
  \emph{A Concise Introduction to Multiagent Systems andDistributed Artificial Intelligence}.
  Morgan \& Claypool, 2007
\bibitem{littman94}
  Michael L. Littman,
  \emph{Markov games as a framework for multi-agent reinforcement learning}.
  1994

\end{thebibliography}

\end{document}
